#include <stdio.h>
#include <math.h>
#include <Windows.h>

#include <cuda.h>
#include "cuda_runtime.h"
#include "device_launch_parameters.h"

#include "../Utils/data_types_common.h"
#include "../Utils/sort_interface.h"
#include "../Utils/cuda.h"
#include "../Utils/host.h"
#include "constants.h"
#include "data_types.h"
#include "kernels.h"
#include "sort_parallel.h"


void QuicksortParallelKeyValue::memoryAllocate(data_t *h_keys, data_t *h_values, uint_t arrayLength)
{
    SortParallelKeyValue::memoryAllocate(h_keys, h_values, arrayLength);

    // Maximum number of sequneces which can get generated by global quicksort. In global quicksort sequences
    // are generated untill total number of sequences is lower than: tableLen / THRESHOLD_PARTITION_SIZE_GLOBAL.
    uint_t maxNumSequences = 2 * ((this->arrayLength - 1) / THRESHOLD_PARTITION_SIZE_GLOBAL + 1);
    // Max number of all thread blocks in GLOBAL quicksort.
    uint_t elemsPerThreadBlock = THREADS_PER_SORT_GLOBAL * ELEMENTS_PER_THREAD_GLOBAL;
    uint_t maxNumThreadBlocks = maxNumSequences * ((THRESHOLD_PARTITION_SIZE_GLOBAL - 1) / elemsPerThreadBlock + 1);
    cudaError_t error;

    /* HOST MEMORY */

    // Sequence metadata memory allocation
    this->h_globalSeqHost = (h_glob_seq_t*)malloc(maxNumSequences * sizeof(*(this->h_globalSeqHost)));
    checkMallocError(this->h_globalSeqHost);
    this->h_globalSeqHostBuffer = (h_glob_seq_t*)malloc(maxNumSequences * sizeof(*(this->h_globalSeqHostBuffer)));
    checkMallocError(this->h_globalSeqHostBuffer);

    // These sequences are transfered between host and device and are therfore allocated in CUDA pinned memory
    error = cudaHostAlloc(
        (void**)&this->h_minMaxValues, 2 * THRESHOLD_REDUCTION * sizeof(*(this->h_minMaxValues)),
        cudaHostAllocDefault
    );
    checkCudaError(error);
    error = cudaHostAlloc(
        (void**)&this->h_globalSeqDev, maxNumSequences * sizeof(*(this->h_globalSeqDev)), cudaHostAllocDefault
    );
    checkCudaError(error);
    error = cudaHostAlloc(
        (void**)&this->h_globalSeqIndexes, maxNumThreadBlocks * sizeof(*(this->h_globalSeqIndexes)),
        cudaHostAllocDefault
    );
    checkCudaError(error);
    error = cudaHostAlloc(
        (void**)&this->h_localSeq, maxNumSequences * sizeof(*(this->h_localSeq)), cudaHostAllocDefault
    );
    checkCudaError(error);

    /* DEVICE MEMORY */

    error = cudaMalloc((void**)&this->d_keysBuffer, this->arrayLength * sizeof(*(this->d_keysBuffer)));
    checkCudaError(error);
    error = cudaMalloc((void**)&this->d_valuesBuffer, this->arrayLength * sizeof(*(this->d_valuesBuffer)));
    checkCudaError(error);
    error = cudaMalloc((void**)&this->d_pivotValues, this->arrayLength * sizeof(*(this->d_pivotValues)));
    checkCudaError(error);

    // Sequence metadata memory allocation
    error = cudaMalloc((void**)&this->d_globalSeqDev, maxNumSequences * sizeof(*(this->d_globalSeqDev)));
    checkCudaError(error);
    error = cudaMalloc((void**)&this->d_globalSeqIndexes, maxNumThreadBlocks * sizeof(*(this->d_globalSeqIndexes)));
    checkCudaError(error);
    error = cudaMalloc((void**)&this->d_localSeq, maxNumSequences * sizeof(*(this->d_localSeq)));
    checkCudaError(error);
}

void QuicksortParallelKeyValue::memoryDestroy()
{
    SortParallelKeyValue::memoryDestroy();
    cudaError_t error;

    /* HOST MEMORY */

    free(this->h_globalSeqHost);
    free(this->h_globalSeqHostBuffer);

    // These arrays are allocated in CUDA pinned memory
    error = cudaFreeHost(this->h_minMaxValues);
    checkCudaError(error);
    error = cudaFreeHost(this->h_globalSeqDev);
    checkCudaError(error);
    error = cudaFreeHost(this->h_globalSeqIndexes);
    checkCudaError(error);
    error = cudaFreeHost(this->h_localSeq);
    checkCudaError(error);

    /* DEVICE MEMORY */
    error = cudaFree(this->d_keysBuffer);
    checkCudaError(error);
    error = cudaFree(this->d_valuesBuffer);
    checkCudaError(error);
    error = cudaFree(this->d_pivotValues);
    checkCudaError(error);

    error = cudaFree(this->d_globalSeqDev);
    checkCudaError(error);
    error = cudaFree(this->d_globalSeqIndexes);
    checkCudaError(error);
    error = cudaFree(this->d_localSeq);
    checkCudaError(error);
}

/*
Ordered array has to be copied from buffer array, not primary array.
*/
void QuicksortParallelKeyValue::memoryCopyDeviceToHost(data_t *h_keys, data_t *h_values, uint_t arrayLength)
{
    cudaError_t error;

    error = cudaMemcpy(
        h_keys, (void *)this->d_keysBuffer, arrayLength * sizeof(*h_keys), cudaMemcpyDeviceToHost
    );
    checkCudaError(error);
    error = cudaMemcpy(
        h_values, (void *)this->d_valuesBuffer, arrayLength * sizeof(*h_values), cudaMemcpyDeviceToHost
    );
    checkCudaError(error);
}

/*
Executes kernel for finding min/max values. Every thread block searches for min/max values in their
corresponding chunk of data. This means kernel will return a list of min/max values with same length
as number of thread blocks executing in kernel.
*/
uint_t QuicksortParallelKeyValue::runMinMaxReductionKernel()
{
    // Half of the array for min values and the other half for max values
    uint_t sharedMemSize = 2 * THREADS_PER_REDUCTION * sizeof(data_t);
    dim3 dimGrid((this->arrayLength - 1) / (THREADS_PER_REDUCTION * ELEMENTS_PER_THREAD_REDUCTION) + 1, 1, 1);
    dim3 dimBlock(THREADS_PER_REDUCTION, 1, 1);

    minMaxReductionKernel<<<dimGrid, dimBlock, sharedMemSize>>>(
        this->d_keys, this->d_keysBuffer, this->arrayLength
    );

    return dimGrid.x;
}

/*
Searches for min/max values in array.
*/
void QuicksortParallelKeyValue::minMaxReduction(data_t &minVal, data_t &maxVal)
{
    minVal = MAX_VAL;
    maxVal = MIN_VAL;

    // Checks whether array is short enough to be reduced entirely on host or if reduction on device is needed
    if (this->arrayLength > THRESHOLD_REDUCTION)
    {
        // Kernel returns array with min/max values of length numVales
        uint_t numValues = runMinMaxReductionKernel();

        cudaError_t error = cudaMemcpy(
            h_minMaxValues, this->d_keysBuffer, 2 * numValues * sizeof(*(this->h_minMaxValues)),
            cudaMemcpyDeviceToHost
        );
        checkCudaError(error);

        data_t *minValues = h_minMaxValues;
        data_t *maxValues = h_minMaxValues + numValues;

        // Finnishes reduction on host
        for (uint_t i = 0; i < numValues; i++)
        {
            minVal = min(minVal, minValues[i]);
            maxVal = max(maxVal, maxValues[i]);
        }
    }
    else
    {
        for (uint_t i = 0; i < this->arrayLength; i++)
        {
            minVal = min(minVal, this->h_keys[i]);
            maxVal = max(maxVal, this->h_keys[i]);
        }
    }
}

/*
Runs global (multiple thread blocks process one sequence) quicksort and coppies required data to and
from device.
*/
void QuicksortParallelKeyValue::runQuickSortGlobalKernel(uint_t numSeqGlobal, uint_t threadBlockCounter)
{
    cudaError_t error;

    // 1. arg: Size of array for calculation of min/max value ("2" because of MIN and MAX)
    // 2. arg: Size of array needed to perform scan of counters for number of elements lower/greater than
    //         pivot ("2" because of intra-warp scan).
    uint_t sharedMemSize = max(
        2 * THREADS_PER_SORT_GLOBAL * sizeof(data_t), 2 * THREADS_PER_SORT_GLOBAL * sizeof(uint_t)
    );
    dim3 dimGrid(threadBlockCounter, 1, 1);
    dim3 dimBlock(THREADS_PER_SORT_GLOBAL, 1, 1);

    error = cudaMemcpy(
        this->d_globalSeqDev, this->h_globalSeqDev, numSeqGlobal * sizeof(*(this->d_globalSeqDev)),
        cudaMemcpyHostToDevice
    );
    checkCudaError(error);
    error = cudaMemcpy(
        this->d_globalSeqIndexes, this->h_globalSeqIndexes, threadBlockCounter * sizeof(*(this->d_globalSeqIndexes)),
        cudaMemcpyHostToDevice
    );
    checkCudaError(error);

    quickSortGlobalKernel<<<dimGrid, dimBlock, sharedMemSize>>>(
        this->d_keys, this->d_values, this->d_keysBuffer, this->d_valuesBuffer, this->d_pivotValues,
        this->d_globalSeqDev, this->d_globalSeqIndexes
    );

    error = cudaMemcpy(
        this->h_globalSeqDev, this->d_globalSeqDev, numSeqGlobal * sizeof(*(this->h_globalSeqDev)),
        cudaMemcpyDeviceToHost
    );
    checkCudaError(error);
}

/*
Finishes quicksort with local (one thread block processes one block) quicksort.
*/
void QuicksortParallelKeyValue::runQuickSortLocalKernel(uint_t numThreadBlocks)
{
    cudaError_t error;

    // The same shared memory array is used for counting elements greater/lower than pivot and for bitonic sort.
    // max(intra-block scan array size, array size for bitonic sort ("2 *" because of key-value pairs))
    uint_t sharedMemSize = max(
        2 * THREADS_PER_SORT_LOCAL * sizeof(uint_t), 2 * THRESHOLD_BITONIC_SORT_LOCAL * sizeof(*(this->d_keys))
    );
    dim3 dimGrid(numThreadBlocks, 1, 1);
    dim3 dimBlock(THREADS_PER_SORT_LOCAL, 1, 1);

    error = cudaMemcpy(
        this->d_localSeq, this->h_localSeq, numThreadBlocks * sizeof(*(this->d_localSeq)), cudaMemcpyHostToDevice
    );
    checkCudaError(error);

    if (sortOrder == ORDER_ASC)
    {
        quickSortLocalKernel<ORDER_ASC><<<dimGrid, dimBlock, sharedMemSize>>>(
            this->d_keys, this->d_values, this->d_keysBuffer, this->d_valuesBuffer, this->d_pivotValues,
            this->d_localSeq
        );
    }
    else
    {
        quickSortLocalKernel<ORDER_DESC><<<dimGrid, dimBlock, sharedMemSize>>>(
            this->d_keys, this->d_values, this->d_keysBuffer, this->d_valuesBuffer, this->d_pivotValues,
            this->d_localSeq
        );
    }
}

/*
Executes parallel quicksort.
*/
void QuicksortParallelKeyValue::sortPrivate()
{
    // Because a lot of empty sequences can be generated, this counter is used to keep track of all
    // theoretically generated sequences.
    uint_t numSeqAll = 1;
    uint_t numSeqGlobal = 1; // Number of sequences for GLOBAL quicksort
    uint_t numSeqLocal = 0;  // Number of sequences for LOCAL quicksort
    uint_t numSeqLimit = (this->arrayLength - 1) / THRESHOLD_PARTITION_SIZE_GLOBAL + 1;
    uint_t elemsPerThreadBlock = THREADS_PER_SORT_GLOBAL * ELEMENTS_PER_THREAD_GLOBAL;
    bool generateSequences = this->arrayLength > THRESHOLD_PARTITION_SIZE_GLOBAL;
    data_t minVal, maxVal;

    // Searches for min and max value in input array
    minMaxReduction(minVal, maxVal);
    // Null/zero distribution
    if (minVal == maxVal)
    {
        data_t *temp = this->d_keys;
        this->d_keys = this->d_keysBuffer;
        this->d_keysBuffer = temp;

        temp = this->d_values;
        this->d_values = this->d_valuesBuffer;
        this->d_valuesBuffer = temp;
        return;
    }
    this->h_globalSeqHost[0].setInitSeq(this->arrayLength, minVal, maxVal);

    // GLOBAL QUICKSORT
    while (generateSequences)
    {
        uint_t threadBlockCounter = 0;

        // Transfers host sequences to device sequences (device needs different data about sequence than host)
        for (uint_t seqIdx = 0; seqIdx < numSeqGlobal; seqIdx++)
        {
            uint_t threadBlocksPerSeq = (h_globalSeqHost[seqIdx].length - 1) / elemsPerThreadBlock + 1;
            this->h_globalSeqDev[seqIdx].setFromHostSeq(
                this->h_globalSeqHost[seqIdx], threadBlockCounter, threadBlocksPerSeq
            );

            // For all thread blocks in current iteration marks, they are assigned to current sequence.
            for (uint_t blockIdx = 0; blockIdx < threadBlocksPerSeq; blockIdx++)
            {
                this->h_globalSeqIndexes[threadBlockCounter++] = seqIdx;
            }
        }

        runQuickSortGlobalKernel(numSeqGlobal, threadBlockCounter);

        uint_t numSeqGlobalOld = numSeqGlobal;
        numSeqGlobal = 0;
        numSeqAll *= 2;

        // Generates new sub-sequences and depending on their size adds them to list for GLOBAL or LOCAL quicksort
        // If theoretical number of sequences reached limit, sequences are transfered to array for LOCAL quicksort
        for (uint_t seqIdx = 0; seqIdx < numSeqGlobalOld; seqIdx++)
        {
            h_glob_seq_t seqHost = this->h_globalSeqHost[seqIdx];
            d_glob_seq_t seqDev = this->h_globalSeqDev[seqIdx];

            // New subsequece (lower)
            if (seqDev.offsetLower > THRESHOLD_PARTITION_SIZE_GLOBAL && numSeqAll < numSeqLimit)
            {
                this->h_globalSeqHostBuffer[numSeqGlobal++].setLowerSeq(seqHost, seqDev);
            }
            else if (seqDev.offsetLower > 0)
            {
                this->h_localSeq[numSeqLocal++].setLowerSeq(seqHost, seqDev);
            }

            // New subsequece (greater)
            if (seqDev.offsetGreater > THRESHOLD_PARTITION_SIZE_GLOBAL && numSeqAll < numSeqLimit)
            {
                this->h_globalSeqHostBuffer[numSeqGlobal++].setGreaterSeq(seqHost, seqDev);
            }
            else if (seqDev.offsetGreater > 0)
            {
                this->h_localSeq[numSeqLocal++].setGreaterSeq(seqHost, seqDev);
            }
        }

        h_glob_seq_t *temp = h_globalSeqHost;
        h_globalSeqHost = h_globalSeqHostBuffer;
        h_globalSeqHostBuffer = temp;

        generateSequences &= numSeqAll < numSeqLimit && numSeqGlobal > 0;
    }

    // If global quicksort was not used, than sequence is initialized for LOCAL quicksort
    if (this->arrayLength <= THRESHOLD_PARTITION_SIZE_GLOBAL)
    {
        numSeqLocal++;
        this->h_localSeq[0].setInitSeq(this->arrayLength);
    }

    runQuickSortLocalKernel(numSeqLocal);
}
