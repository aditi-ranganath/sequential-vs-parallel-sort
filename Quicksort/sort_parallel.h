#ifndef QUICKSORT_PARALLEL_H
#define QUICKSORT_PARALLEL_H

#include "../Utils/data_types_common.h"
#include "../Utils/sort_interface.h"
#include "constants.h"
#include "data_types.h"


/*
Base class for parallel bitonic sort.
Needed for template specialization.

Template params:
_Ko - Key-only
_Kv - Key-value

TODO implement DESC ordering.
*/
template <
    uint_t useReductionInGlobalSort, uint_t thresholdParallelReduction,
    uint_t threadsReduction, uint_t elemsReduction,
    uint_t threasholdPartitionGlobalKo, uint_t threasholdPartitionGlobalKv,
    uint_t threadsSortGlobalKo, uint_t elemsSortGlobalKo,
    uint_t threadsSortGlobalKv, uint_t elemsSortGlobalKv,
    uint_t thresholdBitonicSortKo, uint_t thresholdBitonicSortKv,
    uint_t threadsSortLocalKo, uint_t threadsSortLocalKv
>
class QuicksortParallelBase : public SortParallel
{
protected:
    std::string _sortName = "Quicksort parallel";
    // Device buffer for keys and values
    data_t *_d_keysBuffer, *_d_valuesBuffer;
    // When pivots are scattered in global and local quicksort, they have to be considered as unique elements
    // because of array of values (alongside keys). Because array can contain duplicate keys, values have to
    // be stored in buffer, because end position of pivots isn't known until last thread block processes sequence.
    data_t *_d_valuesPivot;
    // When initial min/max parallel reduction reduces data to threashold, min/max values are coppied to host
    // and reduction is finnished on host. Multiplier "2" is used because of min and max values.
    data_t *_h_minMaxValues;
    // Sequences metadata for GLOBAL quicksort on HOST
    h_glob_seq_t *_h_globalSeqHost, *_h_globalSeqHostBuffer;
    // Sequences metadata for GLOBAL quicksort on DEVICE
    d_glob_seq_t *_h_globalSeqDev, *_d_globalSeqDev;
    // Array of sequence indexes for thread blocks in GLOBAL quicksort. This way thread blocks know which
    // sequence they have to partition.
    uint_t *_h_globalSeqIndexes, *_d_globalSeqIndexes;
    // Sequences metadata for LOCAL quicksort
    loc_seq_t *_h_localSeq, *_d_localSeq;

    void memoryAllocate(data_t *h_keys, data_t *h_values, uint_t arrayLength)
    {
        SortParallel::memoryAllocate(h_keys, h_values, arrayLength);

        // Min/Max calculations needed, because mamory is allocated both for key only and for key-value sort
        uint_t minPartitionSizeGlobal = min(threasholdPartitionGlobalKo, threasholdPartitionGlobalKv);
        uint_t maxPartitionSizeGlobal = max(threasholdPartitionGlobalKo, threasholdPartitionGlobalKv);
        uint_t minElemsPerThreadBlock = min(
            threadsSortGlobalKo * elemsSortGlobalKo, threadsSortGlobalKv * elemsSortGlobalKv
        );

        // Maximum number of sequneces which can get generated by global quicksort. In global quicksort sequences
        // are generated untill total number of sequences is lower than: tableLen / THRESHOLD_PARTITION_SIZE_GLOBAL.
        uint_t maxNumSequences = 2 * ((arrayLength - 1) / minPartitionSizeGlobal + 1);
        // Max number of all thread blocks in GLOBAL quicksort.
        uint_t maxNumThreadBlocks = maxNumSequences * ((maxPartitionSizeGlobal - 1) / minElemsPerThreadBlock + 1);
        cudaError_t error;

        /* HOST MEMORY */

        // Sequence metadata memory allocation
        _h_globalSeqHost = (h_glob_seq_t*)malloc(maxNumSequences * sizeof(*_h_globalSeqHost));
        checkMallocError(_h_globalSeqHost);
        _h_globalSeqHostBuffer = (h_glob_seq_t*)malloc(maxNumSequences * sizeof(*_h_globalSeqHostBuffer));
        checkMallocError(_h_globalSeqHostBuffer);

        // These sequences are transfered between host and device and are therfore allocated in CUDA pinned memory
        error = cudaHostAlloc(
            (void **)&_h_minMaxValues, 2 * thresholdParallelReduction * sizeof(*_h_minMaxValues),
            cudaHostAllocDefault
        );
        checkCudaError(error);
        error = cudaHostAlloc(
            (void **)&_h_globalSeqDev, maxNumSequences * sizeof(*_h_globalSeqDev), cudaHostAllocDefault
        );
        checkCudaError(error);
        error = cudaHostAlloc(
            (void **)&_h_globalSeqIndexes, maxNumThreadBlocks * sizeof(*_h_globalSeqIndexes), cudaHostAllocDefault
        );
        checkCudaError(error);
        error = cudaHostAlloc(
            (void **)&_h_localSeq, maxNumSequences * sizeof(*_h_localSeq), cudaHostAllocDefault
        );
        checkCudaError(error);

        /* DEVICE_MEMORY */

        error = cudaMalloc((void **)&_d_keysBuffer, arrayLength * sizeof(*_d_keysBuffer));
        checkCudaError(error);
        error = cudaMalloc((void **)&_d_valuesBuffer, arrayLength * sizeof(*_d_valuesBuffer));
        checkCudaError(error);
        error = cudaMalloc((void **)&_d_valuesPivot, arrayLength * sizeof(*_d_valuesPivot));
        checkCudaError(error);

        // Sequence metadata memory allocation
        error = cudaMalloc((void **)&_d_globalSeqDev, maxNumSequences * sizeof(*_d_globalSeqDev));
        checkCudaError(error);
        error = cudaMalloc((void **)&_d_globalSeqIndexes, maxNumThreadBlocks * sizeof(*_d_globalSeqIndexes));
        checkCudaError(error);
        error = cudaMalloc((void **)&_d_localSeq, maxNumSequences * sizeof(*_d_localSeq));
        checkCudaError(error);
    }

    /*
    Copies data from device to host. If sorting keys only, than "h_values" contains NULL.
    Result is contained in buffer arrays, not in primary array.
    */
    void memoryCopyAfterSort(data_t *h_keys, data_t *h_values, uint_t arrayLength)
    {
        cudaError_t error;

        // Copies keys
        error = cudaMemcpy(h_keys, (void *)_d_keysBuffer, _arrayLength * sizeof(*_h_keys), cudaMemcpyDeviceToHost);
        checkCudaError(error);

        if (h_values == NULL)
        {
            return;
        }

        // Copies values
        error = cudaMemcpy(
            h_values, (void *)_d_valuesBuffer, arrayLength * sizeof(*h_values), cudaMemcpyDeviceToHost
        );
        checkCudaError(error);
    }

    void memoryDestroy()
    {
        if (_arrayLength == 0)
        {
            return;
        }

        SortParallel::memoryDestroy();

        cudaError_t error;

        /* HOST MEMORY */

        free(_h_globalSeqHost);
        free(_h_globalSeqHostBuffer);

        // These arrays are allocated in CUDA pinned memory
        error = cudaFreeHost(_h_minMaxValues);
        checkCudaError(error);
        error = cudaFreeHost(_h_globalSeqDev);
        checkCudaError(error);
        error = cudaFreeHost(_h_globalSeqIndexes);
        checkCudaError(error);
        error = cudaFreeHost(_h_localSeq);
        checkCudaError(error);

        /* DEVICE MEMORY */

        error = cudaFree(_d_keysBuffer);
        checkCudaError(error);
        error = cudaFree(_d_valuesBuffer);
        checkCudaError(error);
        error = cudaFree(_d_valuesPivot);
        checkCudaError(error);

        error = cudaFree(_d_globalSeqDev);
        checkCudaError(error);
        error = cudaFree(_d_globalSeqIndexes);
        checkCudaError(error);
        error = cudaFree(_d_localSeq);
        checkCudaError(error);
    }

public:
    std::string getSortName()
    {
        return this->_sortName;
    }
};


/*
Class for parallel quicksort.
*/
class QuicksortParallel : public QuicksortParallelBase<
    USE_REDUCTION_IN_GLOBAL_SORT, THRESHOLD_PARALLEL_REDUCTION,
    THREADS_PER_REDUCTION, ELEMENTS_PER_THREAD_REDUCTION,
    THRESHOLD_PARTITION_SIZE_GLOBAL_KO, THRESHOLD_PARTITION_SIZE_GLOBAL_KV,
    THREADS_PER_SORT_GLOBAL_KO, ELEMENTS_PER_THREAD_GLOBAL_KO,
    THREADS_PER_SORT_GLOBAL_KV, ELEMENTS_PER_THREAD_GLOBAL_KV,
    THRESHOLD_BITONIC_SORT_KO, THRESHOLD_BITONIC_SORT_KV,
    THREADS_PER_SORT_LOCAL_KO, THREADS_PER_SORT_LOCAL_KV
>
{};

#endif
